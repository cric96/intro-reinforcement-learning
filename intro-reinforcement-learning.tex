%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS Beamer series / Bologna FC / Template
% Andrea Omicini
% Alma Mater Studiorum - Universit√† di Bologna
% mailto:andrea.omicini@unibo.it
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{default}}
%
\documentclass[presentation, 9pt]{beamer}\mode<presentation>{\usetheme{AMSBolognaFC}}
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{AMSBolognaFC}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[T1]{fontenc}
\usepackage{wasysym}
\usepackage{amsmath,blkarray}
\usepackage[minted,most]{tcolorbox}
\usepackage{centernot}
\usepackage{fontawesome}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage{hyperref}
\usepackage{multicol}
\setminted[scala]{fontsize=\scriptsize,baselinestretch=1,obeytabs=true, tabsize=2}
\usepackage[ddmmyyyy]{datetime}
\setminted{fontsize=\footnotesize}
\renewcommand{\dateseparator}{}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\version}{1}
\usepackage[
	backend=biber,
	citestyle=authoryear-icomp,
	maxcitenames=1,
	bibstyle=numeric]{biblatex}

	\makeatletter

\addbibresource{biblio.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Reinforcement Learning]
{Reinforcement Learning}
\subtitle{An introduction}
%
%
\author[\sspeaker{Aguzzi}]
{\speaker{Gianluca Aguzzi} \href{mailto:gianluca.aguzzi@unibo.it}{gianluca.aguzzi@unibo.it}}
%
\institute[DISI, Univ.\ Bologna]
{Dipartimento di Informatica -- Scienza e Ingegneria (DISI)\\
\textsc{Alma Mater Studiorum} -- Universit{\`a} di Bologna \\[0.5cm]
\textbf{Talk @} \bold{Advanced School in Artificial Intelligence (ASAI)}}
%
\renewcommand{\dateseparator}{/}
\date[\today]{\today}
%
\AtBeginSection[]
{
  \begin{frame}
  \frametitle{Contents}
  \tableofcontents[currentsubsection, 
	sectionstyle=show/shaded, 
	subsectionstyle=show/shaded]
  \end{frame}
}
\AtBeginSubsection[]
{
  \begin{frame}
  \frametitle{Contents}
  \tableofcontents[currentsubsection, 
	sectionstyle=show/shaded, 
	subsectionstyle=show/shaded]
  \end{frame}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%/////////
\frame{\titlepage}
%/////////
\begin{frame}{Hello World!}
	\begin{columns}
		\begin{column}{0.5\textwidth}
		\centering
		\fbox{\includegraphics[width=0.5\linewidth]{img/me.jpeg}}
		\\
		\vspace{0.2cm}
		\href{https://github.com/cric96}{\faGithub} \,
		\href{https://stackoverflow.com/users/10295847/gianluca-aguzzi}{\faStackOverflow} \,
		\href{https://www.linkedin.com/in/gianluca-aguzzi-265998170/}{\faLinkedin} \,
		\href{https://www.unibo.it/sitoweb/gianluca.aguzzi}{\faGlobe} \,
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item PhD student in Computer Science and Engineering
				\item Research interests:
				\begin{itemize}
					\item Multi-agent systems
					\item Distributed Collective Intellingence
					\item Deep Reinforcement Learning
					\item Multi-agent Reinforcement Learning
					\item Distributed Macro-programming
				\end{itemize}
				%\item Lead developer of \href{https://scafi.github.io/}{ScaFi}
				%\item Scala Lover \& Functional Programming enthusiast
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Resources}
\begin{itemize}
	\item \emph{An Introduction to Reinforcement Learning}, Sutton and Barto,1998
	\begin{itemize}
		\item Available online at \url{http://incompleteideas.net/book/the-book-2nd.html}
	\end{itemize}
	\item \emph{Foundations of Deep Reinforcement Learning: Theory and Practice in Python}, Laura Graesser and Wah Loon Keng, 2020
	\item \emph{Deep Mind Lectures}:
	\begin{itemize}
		\item \textbf{Introduction to Reinforcement Learning with David Silver}: \url{https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver}
		\item \textbf{Reinforcement Learning Lecture Series}: \url{https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021}
	\end{itemize}
\end{itemize}
\end{frame}
%===============================================================================
\section{Introduction}
%===============================================================================
\begin{frame}[plain,c]
	%\frametitle{A first slide}
	\begin{center}
	\Huge What is Reinforcement Learning?
	\end{center}
\end{frame}

\begin{frame}[plain,c]
	%\frametitle{A first slide}
	\begin{center}
	\Huge What is Intelligence?
	\\
	\huge \emph{To able to \textbf{learn} to make \textbf{decisions} to achive \textbf{goals}}
	\end{center}
\end{frame}
\begin{frame}{What is Reinforcement Learning?}
\begin{itemize}
	\item Animals learn by interacting with our environment
	\begin{itemize}
		\item Babies learn how to communicate by interacting with parents
		\item Dogs learn how to behave by following the owner's orders
		\item Me learn how to surfing by falling from the surfboard
	\end{itemize}
	\item Difference from supervised learning:
	\begin{itemize}
		\item \textbf{active} learning (learn by doing)
		\item \textbf{sequential} interaction 
	\end{itemize}
	\item Learning guided by \textbf{goal} (goal-directed)
	\item Learning without examples \faArrowRight \, guided by reward signal
\end{itemize}
\end{frame}

\begin{frame}{Interaction loop}
\centering
\Large An \textbf{agent} interacts with the \textbf{environment} by perceive an \textbf{observation} and take an \textbf{action} accordingly which leads to a \textbf{reward}
\\
\includegraphics[height=6cm]{img/interaction-loop.png}
\\
\Large
\bold{Goal}: maximize the \textbf{cumulative reward} over time
\end{frame}

\begin{frame}{On problem expressiveness: reward hypothesis}
\begin{block}{Reward hypothesis}
\emph{Any goal can be formalized as the outcome of maximizing a cumulative reward}
\begin{itemize}
	\item Reinforcement learning is based on this hypothesis
	\item Ideally, we can formalize any problem as a reinforcement learning problem
\end{itemize}
\end{block}

\begin{alertblock}{Stronger statement: reward is enough}
\emph{intelligence, and its associated abilities, can be understood as subserving the maximisation of reward by an agent acting in its environment.}
\begin{itemize}
	\item Really controversial
\end{itemize}
\end{alertblock}
\end{frame}

\begin{frame}{Examples of Reinforcement Learning problems}
\begin{multicols}{2}
	\begin{itemize}
		\item Learning how to surf
		\item Managing a portfolio of cryptocurrencies
		\item Controlling the battery of an electric car 
		\item Playing chessboard
		\item Solving a cubic cube
	\end{itemize}
	\begin{itemize}
		\item[\faArrowRight] \textbf{Reward:} surfing time on the wave
		\item[\faArrowRight] \textbf{Reward:} money earned
		\item[\faArrowRight] \textbf{Reward:} battery level at the end of the day
		\item[\faArrowRight] \textbf{Reward:} win the game
		\item[\faArrowRight] \textbf{Reward:} solving time 
	\end{itemize}
\end{multicols}
\large
\textbf{NB!} if the goal is learn via environment interaction, then these are all reinforcement learning problems, regardless the algorithm involved
\end{frame}

\begin{frame}{Again, What is Reinforcement Learning}
\begin{itemize}
	\item There are several reasons why we should learn:
	\begin{enumerate}
		\item Find \textbf{solutions}
		\begin{itemize}
			\item A robot that reaches a target
			\item A program that plays chess (really well)
		\end{itemize}
		\item Adapt \textbf{online} (dealing with unknowns)
		\begin{itemize}
			\item A robot that learns how to walk in a new environment
			\item A program that learns how to play a new game
		\end{itemize}
	\end{enumerate}
	\item Reinforcement learning is used in both cases
	\item Adapting online is more challenging, and it is not just generalization (e.g. supervised learning)
\end{itemize}

\end{frame}

\begin{frame}{Motivating real world examples}
\centering
\href{https://www.youtube.com/watch?v=V1eYniJ0Rnk}{\includegraphics[width=0.32\textwidth]{img/atari.png}}
\href{https://www.youtube.com/watch?v=43cO39XBPIA}{\includegraphics[width=0.32\textwidth]{img/robots.png}}
\href{https://www.youtube.com/watch?v=gn4nRCC9TwQ}{\includegraphics[width=0.32\textwidth]{img/ppo.png}}
\end{frame}
\section{Formalisation}
\begin{frame}{Reinforcement Learning: core concept}
Reinforcement learning formalism include:
\begin{itemize}
	\item \textbf{Environment}:
	\begin{itemize}
		\item Typically stochastic and unknow but stationary
		\item stochastic \faArrowRight \, the next state is not fully determined by the current state and action (random component)
		\item Environment dynamics expressed as: $p(s', r | s, a)$ \faArrowRight \, not known by the agent
	\end{itemize}
	\item \textbf{Reward signal} 
	\begin{itemize}
		\item Identifies what is good in the environment (the goal)
	\end{itemize}
	\item \textbf{Agent}, which contain:
	\begin{itemize}	
		\item State
		\item Policy
		\item \emph{Value} function estimation?
	\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{Agent and environment}
\centering
\includegraphics[height=4cm]{img/interaction-loop.png}
\begin{itemize}
	\item Each time step $t$:
	\begin{itemize}
		\item The agent receives an observation $o_t \in \mathcal{O}$ (and a reward $r_t \in \mathbb{R}$)
		\item Executes an action $a_t \in \mathcal{A}$
	\end{itemize}
	\item The environment 
	\begin{itemize}
		\item Receives the action $a_t$
		\item Emits the observation $o_{t+1}$ and the reward $r_{t+1}$
	\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{Rewards}
\begin{itemize}
	\item A \bold{reward} $r_t$ is a scalar feedback signal
	\begin{itemize}
		\item In chess, $r_t = 1$ if the agent wins, $r_t = 0$ otherwise
		\item In a robot, $r_t = 1$ if the robot reaches the target, $r_t = 0$ otherwise
		\item For a portfolio, $r_t$ is the profit
	\end{itemize}
	\item Describes how well the agent is doing at step $t$ (define the goal)
	\item The agent's sole objective is to maximize the discounted cumulative reward (\bold{return})
	\begin{equation*}
	\begin{split}
		G_t & = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots \\
		 & = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
		\end{split}
	\end{equation*}
\end{itemize}
\begin{alertblock}{Why discounted?}
\begin{itemize}
	\item Immediate rewards are more important than future rewards
	\item $\gamma \in [0,1]$ is the discount factor
	\item $\gamma = 0$ \faArrowRight \, myopic agent
	\item $\gamma = 1$ \faArrowRight \, far-sighted agent
\end{itemize}
\end{alertblock}
\end{frame}
\begin{frame}{Agent state}
\begin{itemize}
	\item A full episode is a sequence of state-action-reward tuples (called \bold{trajectory})
	
	\item Example: $\mathcal{H}_T$ = $\{ (s_0, a_0, r_1), (s_1, a_1, r_2), \dots, (s_{T-1}, a_{T-1}, r_T) \}$
	\item Markovian property: the future is independent of the past given the present
	\begin{itemize}
		\item Formula: $p(s_{t+1} | s_t, a_t) = p(s_{t+1} | \mathcal{H}_t, a_t)$
		\item \textbf{NB!}: this means that the state $s_t$ is a \textbf{sufficient statistic} of the future
	\end{itemize}
	\item The environment state can be either:
	\begin{itemize}
		\item \textbf{Fully observable}: the agent knows the state
		\item \textbf{Partially observable}: the agent partially observes environment state 
	\end{itemize}
	\item Today we will assume that the state is \textbf{fully observable} and \textbf{Markovian}
	\item Real case scenario: \textbf{partially observable} and \textbf{non-Markovian}
	\begin{itemize}
		\item Also in that situation, reinforcement learning algorithms can be used (particularly the ones based on deep learning)
	\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{Agent Policy}
\begin{itemize}
	\item The agent's behavior is determined by a \bold{policy} $\pi$
	\item A policy is a mapping from state to action
	\item The policy can be either:
	\begin{itemize}
		\item \textbf{Deterministic}: $\pi: \mathcal{S} \rightarrow \mathcal{A}$
		\item \textbf{Stochastic}: $\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$
	\end{itemize}
	\item The policy is typically represented as a \textbf{lookup table} or a \textbf{neural network}
	\item In both cases, the policy is based of an \textbf{estimation} of the \emph{value} function
\end{itemize}
\end{frame}

\begin{frame}{Value function}
\begin{itemize}
	\item The value function $v_{\pi}(s)$ gives the \textbf{long-term value} of state $s$ under policy $\pi$
	\item The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state
	\item Formally, it can be expressed as:
	\begin{equation*}
		\begin{split}
			v_{\pi}(s) = \mathbb{E}_{\pi} [G_t | S_t = s] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right]	
		\end{split}
	\end{equation*}
	\item the state-action value function $q_{\pi}(s,a)$ gives the \textbf{long-term value} of state-action pair $(s,a)$ under policy $\pi$
	\begin{itemize}
		\item Formally, it can be expressed as:
		\begin{equation*}
			\begin{split}
				q_{\pi}(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right]
			\end{split}
		\end{equation*}
	\end{itemize}
\end{itemize}
\end{frame}
%===============================================================================
\section*{}
%===============================================================================

%/////////
\frame{\titlepage}
%/////////

%===============================================================================
\section*{\refname}
%===============================================================================

%%%%
\setbeamertemplate{page number in head/foot}{}
%/////////
\begin{frame}[c,noframenumbering, allowframebreaks]{\refname}
%\begin{frame}[t,allowframebreaks,noframenumbering]{\refname}
	\tiny
	\nocite{*}
	\printbibliography
\end{frame}
%/////////

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
